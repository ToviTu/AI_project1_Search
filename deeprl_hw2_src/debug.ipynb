{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 538/1000000 Episode 1: Total reward: 140.0 Explore P: 0.9995167\n",
      "Step: 976/1000000 Episode 2: Total reward: 135.0 Explore P: 0.9991225\n",
      "Step: 1267/1000000 Episode 3: Total reward: 65.0 Explore P: 0.9988606\n",
      "Step: 1706/1000000 Episode 4: Total reward: 45.0 Explore P: 0.9984655\n",
      "Step: 1990/1000000 Episode 5: Total reward: 80.0 Explore P: 0.9982099\n",
      "Step: 2397/1000000 Episode 6: Total reward: 110.0 Explore P: 0.9978436\n",
      "Step: 2697/1000000 Episode 7: Total reward: 80.0 Explore P: 0.9975736\n",
      "Step: 3116/1000000 Episode 8: Total reward: 125.0 Explore P: 0.9971965\n",
      "Step: 3459/1000000 Episode 9: Total reward: 65.0 Explore P: 0.9968878\n",
      "Step: 3853/1000000 Episode 10: Total reward: 80.0 Explore P: 0.9965332\n",
      "Step: 4454/1000000 Episode 11: Total reward: 180.0 Explore P: 0.9959923\n",
      "Step: 4808/1000000 Episode 12: Total reward: 35.0 Explore P: 0.9956737\n",
      "Step: 5201/1000000 Episode 13: Total reward: 90.0 Explore P: 0.99532\n",
      "Step: 5640/1000000 Episode 14: Total reward: 65.0 Explore P: 0.9949249\n",
      "Step: 6210/1000000 Episode 15: Total reward: 150.0 Explore P: 0.9944119\n",
      "Step: 6542/1000000 Episode 16: Total reward: 65.0 Explore P: 0.9941131\n",
      "Step: 7087/1000000 Episode 17: Total reward: 105.0 Explore P: 0.9936226\n",
      "Step: 7454/1000000 Episode 18: Total reward: 75.0 Explore P: 0.9932923\n",
      "Step: 7951/1000000 Episode 19: Total reward: 80.0 Explore P: 0.992845\n",
      "Step: 8810/1000000 Episode 20: Total reward: 230.0 Explore P: 0.9920719\n",
      "Step: 9624/1000000 Episode 21: Total reward: 230.0 Explore P: 0.9913393\n",
      "Step: 10148/1000000 Episode 22: Total reward: 140.0 Explore P: 0.9908677\n",
      "Step: 10751/1000000 Episode 23: Total reward: 210.0 Explore P: 0.990325\n",
      "Step: 11048/1000000 Episode 24: Total reward: 80.0 Explore P: 0.9900577\n",
      "Step: 11449/1000000 Episode 25: Total reward: 30.0 Explore P: 0.9896968\n",
      "Step: 11814/1000000 Episode 26: Total reward: 120.0 Explore P: 0.9893683\n",
      "Step: 12648/1000000 Episode 27: Total reward: 315.0 Explore P: 0.9886177\n",
      "Step: 13340/1000000 Episode 28: Total reward: 455.0 Explore P: 0.9879949\n",
      "Step: 13634/1000000 Episode 29: Total reward: 80.0 Explore P: 0.9877303\n",
      "Step: 13937/1000000 Episode 30: Total reward: 55.0 Explore P: 0.9874576\n",
      "Step: 14401/1000000 Episode 31: Total reward: 155.0 Explore P: 0.98704\n",
      "Step: 14689/1000000 Episode 32: Total reward: 45.0 Explore P: 0.9867808\n",
      "Step: 15108/1000000 Episode 33: Total reward: 155.0 Explore P: 0.9864037\n",
      "Step: 15512/1000000 Episode 34: Total reward: 80.0 Explore P: 0.9860401\n",
      "Step: 15976/1000000 Episode 35: Total reward: 155.0 Explore P: 0.9856225\n",
      "Step: 16770/1000000 Episode 36: Total reward: 205.0 Explore P: 0.9849079\n",
      "Step: 17383/1000000 Episode 37: Total reward: 210.0 Explore P: 0.9843562\n",
      "Step: 17673/1000000 Episode 38: Total reward: 80.0 Explore P: 0.9840952\n",
      "Step: 18197/1000000 Episode 39: Total reward: 120.0 Explore P: 0.9836236\n",
      "Step: 18564/1000000 Episode 40: Total reward: 65.0 Explore P: 0.9832933\n",
      "Step: 18856/1000000 Episode 41: Total reward: 30.0 Explore P: 0.9830305\n",
      "Step: 19254/1000000 Episode 42: Total reward: 110.0 Explore P: 0.9826722999999999\n",
      "Step: 19591/1000000 Episode 43: Total reward: 60.0 Explore P: 0.982369\n",
      "Step: 19938/1000000 Episode 44: Total reward: 50.0 Explore P: 0.9820567\n",
      "Step: 20571/1000000 Episode 45: Total reward: 385.0 Explore P: 0.981487\n",
      "Step: 20980/1000000 Episode 46: Total reward: 60.0 Explore P: 0.9811189\n",
      "Step: 21662/1000000 Episode 47: Total reward: 125.0 Explore P: 0.9805051\n",
      "Step: 22042/1000000 Episode 48: Total reward: 125.0 Explore P: 0.9801631\n",
      "Step: 22690/1000000 Episode 49: Total reward: 210.0 Explore P: 0.9795799\n",
      "Step: 23226/1000000 Episode 50: Total reward: 240.0 Explore P: 0.9790975\n",
      "Step: 23748/1000000 Episode 51: Total reward: 110.0 Explore P: 0.9786277\n",
      "Step: 24051/1000000 Episode 52: Total reward: 90.0 Explore P: 0.978355\n",
      "Step: 24338/1000000 Episode 53: Total reward: 85.0 Explore P: 0.9780967\n",
      "Step: 24763/1000000 Episode 54: Total reward: 180.0 Explore P: 0.9777142\n",
      "Step: 25431/1000000 Episode 55: Total reward: 290.0 Explore P: 0.977113\n",
      "Step: 25920/1000000 Episode 56: Total reward: 110.0 Explore P: 0.9766729\n",
      "Step: 26552/1000000 Episode 57: Total reward: 65.0 Explore P: 0.9761041\n",
      "Step: 27113/1000000 Episode 58: Total reward: 290.0 Explore P: 0.9755992\n",
      "Step: 27759/1000000 Episode 59: Total reward: 185.0 Explore P: 0.9750178\n",
      "Step: 28365/1000000 Episode 60: Total reward: 190.0 Explore P: 0.9744724\n",
      "Step: 29023/1000000 Episode 61: Total reward: 415.0 Explore P: 0.9738802\n",
      "Step: 29398/1000000 Episode 62: Total reward: 50.0 Explore P: 0.9735427\n",
      "Step: 30005/1000000 Episode 63: Total reward: 210.0 Explore P: 0.9729964\n",
      "Step: 30767/1000000 Episode 64: Total reward: 460.0 Explore P: 0.9723106\n",
      "Step: 31110/1000000 Episode 65: Total reward: 65.0 Explore P: 0.9720019\n",
      "Step: 31567/1000000 Episode 66: Total reward: 185.0 Explore P: 0.9715906\n",
      "Step: 32063/1000000 Episode 67: Total reward: 100.0 Explore P: 0.9711442\n",
      "Step: 32587/1000000 Episode 68: Total reward: 135.0 Explore P: 0.9706726\n",
      "Step: 33325/1000000 Episode 69: Total reward: 345.0 Explore P: 0.9700084\n",
      "Step: 33712/1000000 Episode 70: Total reward: 155.0 Explore P: 0.9696601\n",
      "Step: 34276/1000000 Episode 71: Total reward: 110.0 Explore P: 0.9691525\n",
      "Step: 34842/1000000 Episode 72: Total reward: 355.0 Explore P: 0.9686431\n",
      "Step: 35271/1000000 Episode 73: Total reward: 80.0 Explore P: 0.968257\n",
      "Step: 35851/1000000 Episode 74: Total reward: 155.0 Explore P: 0.967735\n",
      "Step: 36578/1000000 Episode 75: Total reward: 165.0 Explore P: 0.9670807\n",
      "Step: 36870/1000000 Episode 76: Total reward: 55.0 Explore P: 0.9668179\n",
      "Step: 37635/1000000 Episode 77: Total reward: 335.0 Explore P: 0.9661294\n",
      "Step: 38321/1000000 Episode 78: Total reward: 215.0 Explore P: 0.965512\n",
      "Step: 38577/1000000 Episode 79: Total reward: 45.0 Explore P: 0.9652816\n",
      "Step: 39218/1000000 Episode 80: Total reward: 165.0 Explore P: 0.9647047\n",
      "Step: 39690/1000000 Episode 81: Total reward: 180.0 Explore P: 0.9642799\n",
      "Step: 40293/1000000 Episode 82: Total reward: 225.0 Explore P: 0.9637372\n",
      "Step: 40879/1000000 Episode 83: Total reward: 145.0 Explore P: 0.9632098\n",
      "Step: 41395/1000000 Episode 84: Total reward: 120.0 Explore P: 0.9627454\n",
      "Step: 41963/1000000 Episode 85: Total reward: 120.0 Explore P: 0.9622342\n",
      "Step: 42635/1000000 Episode 86: Total reward: 270.0 Explore P: 0.9616294\n",
      "Step: 43072/1000000 Episode 87: Total reward: 120.0 Explore P: 0.9612361\n",
      "Step: 43749/1000000 Episode 88: Total reward: 235.0 Explore P: 0.9606268\n",
      "Step: 44232/1000000 Episode 89: Total reward: 150.0 Explore P: 0.9601921\n",
      "Step: 44692/1000000 Episode 90: Total reward: 170.0 Explore P: 0.9597781\n",
      "Step: 45144/1000000 Episode 91: Total reward: 110.0 Explore P: 0.9593713\n",
      "Step: 45573/1000000 Episode 92: Total reward: 125.0 Explore P: 0.9589852\n",
      "Step: 46036/1000000 Episode 93: Total reward: 180.0 Explore P: 0.9585685\n",
      "Step: 46441/1000000 Episode 94: Total reward: 110.0 Explore P: 0.9582040000000001\n",
      "Step: 46982/1000000 Episode 95: Total reward: 155.0 Explore P: 0.9577171\n",
      "Step: 47424/1000000 Episode 96: Total reward: 50.0 Explore P: 0.9573193\n",
      "Step: 48228/1000000 Episode 97: Total reward: 465.0 Explore P: 0.9565957\n",
      "Step: 49031/1000000 Episode 98: Total reward: 410.0 Explore P: 0.955873\n",
      "Step: 49519/1000000 Episode 99: Total reward: 30.0 Explore P: 0.9554338\n",
      "Step: 50097/1000000 Episode 100: Total reward: 185.0 Explore P: 0.9549136\n",
      "Step: 50400/1000000 Episode 101: Total reward: 60.0 Explore P: 0.9546409\n",
      "Step: 50960/1000000 Episode 102: Total reward: 185.0 Explore P: 0.9541369\n",
      "Step: 51485/1000000 Episode 103: Total reward: 105.0 Explore P: 0.9536644\n",
      "Step: 52121/1000000 Episode 104: Total reward: 165.0 Explore P: 0.953092\n",
      "Step: 52669/1000000 Episode 105: Total reward: 130.0 Explore P: 0.9525988\n",
      "Step: 53439/1000000 Episode 106: Total reward: 255.0 Explore P: 0.9519058\n",
      "Step: 53913/1000000 Episode 107: Total reward: 105.0 Explore P: 0.9514792\n",
      "Step: 54370/1000000 Episode 108: Total reward: 155.0 Explore P: 0.9510679\n",
      "Step: 54920/1000000 Episode 109: Total reward: 125.0 Explore P: 0.9505729\n",
      "Step: 55368/1000000 Episode 110: Total reward: 105.0 Explore P: 0.9501697\n",
      "Step: 55846/1000000 Episode 111: Total reward: 45.0 Explore P: 0.9497395\n",
      "Step: 56420/1000000 Episode 112: Total reward: 140.0 Explore P: 0.9492229\n",
      "Step: 56786/1000000 Episode 113: Total reward: 45.0 Explore P: 0.9488935000000001\n",
      "Step: 57401/1000000 Episode 114: Total reward: 145.0 Explore P: 0.94834\n",
      "Step: 57759/1000000 Episode 115: Total reward: 45.0 Explore P: 0.9480178\n",
      "Step: 58474/1000000 Episode 116: Total reward: 265.0 Explore P: 0.9473743\n",
      "Step: 58765/1000000 Episode 117: Total reward: 80.0 Explore P: 0.9471124\n",
      "Step: 59141/1000000 Episode 118: Total reward: 60.0 Explore P: 0.946774\n",
      "Step: 59619/1000000 Episode 119: Total reward: 120.0 Explore P: 0.9463438\n",
      "Step: 60286/1000000 Episode 120: Total reward: 325.0 Explore P: 0.9457435\n",
      "Step: 60699/1000000 Episode 121: Total reward: 120.0 Explore P: 0.9453718\n",
      "Step: 61091/1000000 Episode 122: Total reward: 105.0 Explore P: 0.945019\n",
      "Step: 61803/1000000 Episode 123: Total reward: 165.0 Explore P: 0.9443782\n",
      "Step: 62092/1000000 Episode 124: Total reward: 10.0 Explore P: 0.9441181\n",
      "Step: 62674/1000000 Episode 125: Total reward: 155.0 Explore P: 0.9435943\n",
      "Step: 63065/1000000 Episode 126: Total reward: 70.0 Explore P: 0.9432424\n",
      "Step: 63432/1000000 Episode 127: Total reward: 5.0 Explore P: 0.9429121\n",
      "Step: 63987/1000000 Episode 128: Total reward: 165.0 Explore P: 0.9424126\n",
      "Step: 64281/1000000 Episode 129: Total reward: 45.0 Explore P: 0.942148\n",
      "Step: 64812/1000000 Episode 130: Total reward: 155.0 Explore P: 0.9416701\n",
      "Step: 65284/1000000 Episode 131: Total reward: 125.0 Explore P: 0.9412453\n",
      "Step: 65854/1000000 Episode 132: Total reward: 210.0 Explore P: 0.9407323\n",
      "Step: 66464/1000000 Episode 133: Total reward: 130.0 Explore P: 0.9401832999999999\n",
      "Step: 66837/1000000 Episode 134: Total reward: 95.0 Explore P: 0.9398476\n",
      "Step: 67257/1000000 Episode 135: Total reward: 80.0 Explore P: 0.9394696\n",
      "Step: 67662/1000000 Episode 136: Total reward: 75.0 Explore P: 0.9391051\n",
      "Step: 68119/1000000 Episode 137: Total reward: 85.0 Explore P: 0.9386938\n",
      "Step: 69146/1000000 Episode 138: Total reward: 215.0 Explore P: 0.9377695\n",
      "Step: 69687/1000000 Episode 139: Total reward: 105.0 Explore P: 0.9372826\n",
      "Step: 70156/1000000 Episode 140: Total reward: 105.0 Explore P: 0.9368605\n",
      "Step: 70623/1000000 Episode 141: Total reward: 105.0 Explore P: 0.9364402000000001\n",
      "Step: 71063/1000000 Episode 142: Total reward: 65.0 Explore P: 0.9360442\n",
      "Step: 71587/1000000 Episode 143: Total reward: 110.0 Explore P: 0.9355726\n",
      "Step: 71917/1000000 Episode 144: Total reward: 30.0 Explore P: 0.9352756\n",
      "Step: 72518/1000000 Episode 145: Total reward: 130.0 Explore P: 0.9347347\n",
      "Step: 73419/1000000 Episode 146: Total reward: 220.0 Explore P: 0.9339238\n",
      "Step: 73832/1000000 Episode 147: Total reward: 55.0 Explore P: 0.9335521\n",
      "Step: 74915/1000000 Episode 148: Total reward: 215.0 Explore P: 0.9325774\n",
      "Step: 75574/1000000 Episode 149: Total reward: 225.0 Explore P: 0.9319843\n",
      "Step: 75950/1000000 Episode 150: Total reward: 35.0 Explore P: 0.9316459\n",
      "Step: 76286/1000000 Episode 151: Total reward: 40.0 Explore P: 0.9313435\n",
      "Step: 76802/1000000 Episode 152: Total reward: 120.0 Explore P: 0.9308791\n",
      "Step: 77088/1000000 Episode 153: Total reward: 100.0 Explore P: 0.9306217\n",
      "Step: 77545/1000000 Episode 154: Total reward: 155.0 Explore P: 0.9302104\n",
      "Step: 78134/1000000 Episode 155: Total reward: 210.0 Explore P: 0.9296803\n",
      "Step: 78436/1000000 Episode 156: Total reward: 55.0 Explore P: 0.9294085\n",
      "Step: 78939/1000000 Episode 157: Total reward: 110.0 Explore P: 0.9289558\n",
      "Step: 79507/1000000 Episode 158: Total reward: 165.0 Explore P: 0.9284446\n",
      "Step: 80043/1000000 Episode 159: Total reward: 215.0 Explore P: 0.9279622\n",
      "Step: 80589/1000000 Episode 160: Total reward: 110.0 Explore P: 0.9274708\n",
      "Step: 81053/1000000 Episode 161: Total reward: 210.0 Explore P: 0.9270532\n",
      "Step: 81886/1000000 Episode 162: Total reward: 160.0 Explore P: 0.9263035\n",
      "Step: 82547/1000000 Episode 163: Total reward: 225.0 Explore P: 0.9257086\n",
      "Step: 83041/1000000 Episode 164: Total reward: 105.0 Explore P: 0.925264\n",
      "Step: 83733/1000000 Episode 165: Total reward: 140.0 Explore P: 0.9246411999999999\n",
      "Step: 84029/1000000 Episode 166: Total reward: 80.0 Explore P: 0.9243748\n",
      "Step: 84637/1000000 Episode 167: Total reward: 125.0 Explore P: 0.9238276\n",
      "Step: 85449/1000000 Episode 168: Total reward: 350.0 Explore P: 0.9230968\n",
      "Step: 86126/1000000 Episode 169: Total reward: 135.0 Explore P: 0.9224875\n",
      "Step: 86594/1000000 Episode 170: Total reward: 105.0 Explore P: 0.9220663\n",
      "Step: 87222/1000000 Episode 171: Total reward: 165.0 Explore P: 0.9215011\n",
      "Step: 87711/1000000 Episode 172: Total reward: 135.0 Explore P: 0.921061\n",
      "Step: 88110/1000000 Episode 173: Total reward: 35.0 Explore P: 0.9207019\n",
      "Step: 88496/1000000 Episode 174: Total reward: 125.0 Explore P: 0.9203545\n",
      "Step: 88753/1000000 Episode 175: Total reward: 50.0 Explore P: 0.9201232\n",
      "Step: 89088/1000000 Episode 176: Total reward: 30.0 Explore P: 0.9198217\n",
      "Step: 89779/1000000 Episode 177: Total reward: 410.0 Explore P: 0.9191998\n",
      "Step: 90130/1000000 Episode 178: Total reward: 55.0 Explore P: 0.9188839\n",
      "Step: 90512/1000000 Episode 179: Total reward: 75.0 Explore P: 0.9185401\n",
      "Step: 90901/1000000 Episode 180: Total reward: 30.0 Explore P: 0.9181900000000001\n",
      "Step: 91411/1000000 Episode 181: Total reward: 180.0 Explore P: 0.917731\n",
      "Step: 91694/1000000 Episode 182: Total reward: 60.0 Explore P: 0.9174763\n",
      "Step: 92079/1000000 Episode 183: Total reward: 30.0 Explore P: 0.9171298\n",
      "Step: 92517/1000000 Episode 184: Total reward: 75.0 Explore P: 0.9167356\n",
      "Step: 93287/1000000 Episode 185: Total reward: 190.0 Explore P: 0.9160425999999999\n",
      "Step: 93807/1000000 Episode 186: Total reward: 130.0 Explore P: 0.9155746\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/codes/AI_projects/deeprl_hw2_src/dqn_atari.py:173\u001b[0m\n\u001b[1;32m    169\u001b[0m     agent\u001b[38;5;241m.\u001b[39mevaluate(env, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, policy\u001b[38;5;241m=\u001b[39mtfrl\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mGreedyPolicy())\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes/AI_projects/deeprl_hw2_src/dqn_atari.py:162\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(\n\u001b[1;32m    151\u001b[0m     q_network\u001b[38;5;241m=\u001b[39mcreate_model(window, input_shape, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn),\n\u001b[1;32m    152\u001b[0m     preprocessor\u001b[38;5;241m=\u001b[39mPreprocessorSequence(input_shape, window),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatchsize,\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    161\u001b[0m agent\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam, loss_func\u001b[38;5;241m=\u001b[39mmean_huber_loss)\n\u001b[0;32m--> 162\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinearDecayGreedyEpsilonPolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGreedyEpsilonPolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepsilon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_size\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m agent\u001b[38;5;241m.\u001b[39mevaluate(env, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, policy\u001b[38;5;241m=\u001b[39mtfrl\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mGreedyPolicy())\n",
      "File \u001b[0;32m~/codes/AI_projects/deeprl_hw2_src/deeprl_hw2/dqn.py:262\u001b[0m, in \u001b[0;36mDQNAgent.fit\u001b[0;34m(self, env, num_iterations, max_episode_length, policy)\u001b[0m\n\u001b[1;32m    255\u001b[0m processed_next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor\u001b[38;5;241m.\u001b[39mprocess_state_for_memory(\n\u001b[1;32m    256\u001b[0m     next_state,\n\u001b[1;32m    257\u001b[0m     update_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    260\u001b[0m     processed_state, action, reward, processed_next_state, done\n\u001b[1;32m    261\u001b[0m )\n\u001b[0;32m--> 262\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/codes/AI_projects/deeprl_hw2_src/deeprl_hw2/dqn.py:202\u001b[0m, in \u001b[0;36mDQNAgent.update_policy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ(states)  \u001b[38;5;66;03m# (B, A)\u001b[39;00m\n\u001b[1;32m    199\u001b[0m selected_q_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    200\u001b[0m     q_values, \u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    201\u001b[0m )\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# (B, 1) -> (B)\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_q_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/codes/AI_projects/deeprl_hw2_src/deeprl_hw2/objectives.py:65\u001b[0m, in \u001b[0;36mmean_huber_loss\u001b[0;34m(y_true, y_pred, max_grad)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_huber_loss\u001b[39m(y_true, y_pred, max_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return mean huber loss.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    Same as huber_loss, but takes the mean over all values in the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m      The mean huber loss.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mhuber_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean(batch_loss)\n",
      "File \u001b[0;32m~/codes/AI_projects/deeprl_hw2_src/deeprl_hw2/objectives.py:38\u001b[0m, in \u001b[0;36mhuber_loss\u001b[0;34m(y_true, y_pred, max_grad)\u001b[0m\n\u001b[1;32m     32\u001b[0m diff \u001b[38;5;241m=\u001b[39m y_true \u001b[38;5;241m-\u001b[39m y_pred\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Assuming max_grad is a scalar\u001b[39;00m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m     36\u001b[0m     torch\u001b[38;5;241m.\u001b[39mabs(diff) \u001b[38;5;241m<\u001b[39m max_grad,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m diff\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m---> 38\u001b[0m     max_grad \u001b[38;5;241m*\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m max_grad),\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run dqn_atari.py --env \"ALE/SpaceInvaders-v5\" -o ./runs --seed 42"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
